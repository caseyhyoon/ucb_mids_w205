{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Command line script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Command to bring up cluster (and sanity check)\n",
    "\n",
    "    docker-compose up-d\n",
    "    docker-compose ps\n",
    "    docker ps -a\n",
    "    \n",
    "#### Command to create kafka topic -- assessments\n",
    "\n",
    "    docker-compose exec kafka kafka-topics --create --topic assessments --partitions 1 --replication-factor 1 --if-not-exists --zookeeper zookeeper:32181\n",
    "    \n",
    "#### Command to check the kafka topic\n",
    "\n",
    "    docker-compose exec kafka kafka-topics --describe --topic assessments  --zookeeper zookeeper:32181\n",
    "    \n",
    "#### Commands to publish and consume assessments\n",
    "    (Publish)\n",
    "    docker-compose exec mids bash -c \"cat /w205/project-2-caseyhyoon/assessment-attempts-20180128-121051-nested.json | jq '.[]' -c | kafkacat -P -b kafka:29092 -t assessments\"\n",
    "    \n",
    "    (Consume)\n",
    "    docker-compose exec mids bash -c \"kafkacat -C -b kafka:29092 -t assessments -o beginning -e\"\n",
    "    \n",
    "#### Commands to shutdown cluster (and sanity check)\n",
    "\n",
    "    docker-compose down\n",
    "    docker-compose ps\n",
    "    docker ps -a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Business Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What are the top 5 most certified exams?\n",
    "\n",
    "2. What are the hardest exams?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pyspark code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1 What are the top 5 most certified exams?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create dataframe by subscribing kafka topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[key: binary, value: binary, topic: string, partition: int, offset: bigint, timestamp: timestamp, timestampType: int]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assessments = spark.read.format(\"kafka\").option(\"kafka.bootstrap.servers\", \"kafka:29092\").option(\"subscribe\",\"assessments\").option(\"startingOffsets\", \"earliest\").option(\"endingOffsets\", \"latest\").load() \n",
    "assessments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Casting json data as strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assessments.cache()\n",
    "assessments = assessments.select(assessments.value.cast('string'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom lambda function for certifications, unrolling json data, registering temporary tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_lambda_certifications(x):\n",
    "    \n",
    "    raw_dict = json.loads(x.value)\n",
    "    my_list = []\n",
    "    certification = 0\n",
    "    if \"certification\" in raw_dict:\n",
    "        if raw_dict['certification']:\n",
    "            certification += 1\n",
    "    my_dict = {\"exam_name\": raw_dict['exam_name'],\n",
    "               \"certification\": certification}\n",
    "    my_list.append(Row(**my_dict))\n",
    "    \n",
    "    return my_list\n",
    "\n",
    "certifications_assessments = assessments.rdd.flatMap(my_lambda_certifications).toDF()\n",
    "certifications_assessments.registerTempTable('exam_certs')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SQL Query on temporary table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sql\n",
    "SELECT exam_name, SUM(certification) as sum_cert\n",
    "FROM exam_certs\n",
    "GROUP BY exam_name\n",
    "ORDER BY sum_cert DESC\n",
    "LIMIT 5\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+--------+\n",
      "|exam_name                      |sum_cert|\n",
      "+-------------------------------+--------+\n",
      "|Learning Git                   |394     |\n",
      "|Introduction to Python         |162     |\n",
      "|Intermediate Python Programming|158     |\n",
      "|Introduction to Java 8         |158     |\n",
      "|Learning to Program with R     |128     |\n",
      "+-------------------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "most_certified = spark.sql(\"SELECT exam_name, SUM(certification) as sum_cert FROM exam_certs GROUP BY exam_name ORDER BY sum_cert DESC LIMIT 5\")\n",
    "most_certified.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANSWER:\n",
    "    1. Learning Git\n",
    "    2. Introduction to Python\n",
    "    3. Introduction to Java 8\n",
    "    4. Intermediate Python Programming\n",
    "    5. Learning to Program with R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Writing to HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_certified.write.parquet(\"/tmp/most_certified\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2 What are the hardest exams?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom lambda function for difficulty, unrolling json data, registering temporary tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_lambda_difficulty(x):\n",
    "    \n",
    "    raw_dict = json.loads(x.value)\n",
    "    my_list = []\n",
    "    \n",
    "    if \"sequences\" in raw_dict:\n",
    "        \n",
    "        if \"counts\" in raw_dict[\"sequences\"]:\n",
    "            \n",
    "            if \"correct\" in raw_dict[\"sequences\"][\"counts\"] and \"total\" in raw_dict[\"sequences\"][\"counts\"]:\n",
    "                    \n",
    "                my_dict = {\"exam_name\": raw_dict[\"exam_name\"],\n",
    "                           \"correct\": raw_dict[\"sequences\"][\"counts\"][\"correct\"], \n",
    "                           \"total\": raw_dict[\"sequences\"][\"counts\"][\"total\"]}\n",
    "                my_list.append(Row(**my_dict))\n",
    "    \n",
    "    return my_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "difficulty_assessments = assessments.rdd.flatMap(my_lambda_difficulty).toDF()\n",
    "difficulty_assessments.registerTempTable('difficulty')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SQL Query on temporary table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sql\n",
    "SELECT exam_name, avg((total-correct)/total) as incorrect_rating\n",
    "FROM difficulty\n",
    "GROUP BY exam_name\n",
    "ORDER BY incorrect_rating DESC\n",
    "LIMIT 5\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------+------------------+\n",
      "|exam_name                                  |incorrect_rating  |\n",
      "+-------------------------------------------+------------------+\n",
      "|Client-Side Data Storage for Web Developers|0.8               |\n",
      "|Native Web Apps for Android                |0.75              |\n",
      "|View Updating                              |0.75              |\n",
      "|Arduino Prototyping Techniques             |0.6666666666666667|\n",
      "|Mastering Advanced Git                     |0.6397058823529411|\n",
      "+-------------------------------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "difficulty = spark.sql(\"SELECT exam_name, avg((total-correct)/total) as incorrect_rating FROM difficulty GROUP BY exam_name ORDER BY incorrect_rating DESC limit 5\")\n",
    "difficulty.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANSWER:\n",
    "    1. Client-Side Data Storage for Web Developers\n",
    "    2. View Updating\n",
    "    3. Native Web Apps for Android\n",
    "    4. Arduino Prototyping Techniques\n",
    "    5. Mastering Advanced "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Writing to HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "difficulty.write.parquet(\"/tmp/difficulty\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
